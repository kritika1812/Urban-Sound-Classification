{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"UrbanSound8K/metadata/UrbanSound8K.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "from scipy.io import wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_class(filename):\n",
    "    excerpt = data[data['slice_file_name'] == filename]\n",
    "    path_name = os.path.join('UrbanSound8K/audio', 'fold'+str(excerpt.fold.values[0]), filename)\n",
    "    return path_name, excerpt['class'].values[0]\n",
    "  \n",
    "def wav_fmt_parser(file_name):\n",
    "    full_path, _ = path_class(file_name)\n",
    "    wave_file = open(full_path,\"rb\")\n",
    "    riff_fmt = wave_file.read(36)\n",
    "    n_channels_string = riff_fmt[22:24]\n",
    "    n_channels = struct.unpack(\"H\",n_channels_string)[0]\n",
    "    s_rate_string = riff_fmt[24:28]\n",
    "    s_rate = struct.unpack(\"I\",s_rate_string)[0]\n",
    "    bit_depth_string = riff_fmt[-2:]\n",
    "    bit_depth = struct.unpack(\"H\",bit_depth_string)[0]\n",
    "    return (n_channels,s_rate,bit_depth) \n",
    "# n_channels => no. of channels\n",
    "# s_rate => sampling rate\n",
    "# bit_depth => bit depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "      <th>n_channels</th>\n",
       "      <th>sampling_rate</th>\n",
       "      <th>bit_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>2</td>\n",
       "      <td>44100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  n_channels  sampling_rate  bit_depth  \n",
       "0          dog_bark           2          44100         16  \n",
       "1  children_playing           2          44100         16  \n",
       "2  children_playing           2          44100         16  \n",
       "3  children_playing           2          44100         16  \n",
       "4  children_playing           2          44100         16  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_fmt_data = [wav_fmt_parser(i) for i in data.slice_file_name]\n",
    "data[['n_channels','sampling_rate','bit_depth']] = pd.DataFrame(wav_fmt_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# By default, Librosa’s load function will convert the sampling rate to 22.05khz, as well as reducing the number of channels \n",
    "# to 1(mono), and normalise the data so that the values will range from -1 to 1.\n",
    "\n",
    "# Spectrograms are a useful technique for visualising the spectrum of frequencies of a sound and how they vary during a very \n",
    "# short period of time.\n",
    "# **** MFCC ****\n",
    "# The main difference is that a spectrogram uses a linear spaced frequency scale (so each frequency bin is spaced an equal \n",
    "# number of Hertz apart), whereas an MFCC uses a quasi-logarithmic spaced frequency scale, which is more similar to how the\n",
    "# human auditory system processes sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None \n",
    "     \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    file_name = os.path.join('UrbanSound8K','audio', 'fold'+str(row.fold), str(row.slice_file_name))\n",
    "    \n",
    "    class_label = row[\"class\"]\n",
    "    data_feature = extract_features(file_name)\n",
    "    \n",
    "    features.append([data_feature, class_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[[-306.77255, -177.59209, -99.13616, -65.97198...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[[-457.69534, -451.0248, -450.68613, -445.0000...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[[-468.0367, -467.42264, -481.04654, -486.5948...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[[-422.42215, -411.9085, -409.46243, -409.0892...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[[-438.10162, -434.47787, -443.3284, -442.6643...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature       class_label\n",
       "0  [[-306.77255, -177.59209, -99.13616, -65.97198...          dog_bark\n",
       "1  [[-457.69534, -451.0248, -450.68613, -445.0000...  children_playing\n",
       "2  [[-468.0367, -467.42264, -481.04654, -486.5948...  children_playing\n",
       "3  [[-422.42215, -411.9085, -409.46243, -409.0892...  children_playing\n",
       "4  [[-438.10162, -434.47787, -443.3284, -442.6643...  children_playing"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X\n",
    "l = featuresdf.feature.tolist()\n",
    "for i in range(len(l)):\n",
    "    pad_val = 174 - l[i].shape[1]\n",
    "    arr = l[i]\n",
    "    arr = np.pad(arr,((0,0),(0,pad_val)),'constant')\n",
    "    l[i] = arr\n",
    "    #print(l[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 40, 174)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(l)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array(featuresdf.class_label.tolist())\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "lb = LabelEncoder()  # Encode target labels with value between 0 and n_classes-1.\n",
    "y = np_utils.to_categorical(lb.fit_transform(Y))\n",
    "# to_categorical => one hot encode integer data.\n",
    "# label encoder => convert categorical labels to integers 0 to num_classes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a sequential model, starting with a simple model architecture, consisting of four Conv2D convolution layers, with \n",
    "# our final output layer being a dense layer. Our output layer will have 10 nodes (num_labels) which matches the number of \n",
    "# possible classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 40\n",
    "num_columns = 174\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = y.shape[1]\n",
    "filter_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model \n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "# flatten just rearranges elements to convert multi-dimensional to single dimension \n",
    "# GlobalAveragePooling is a methodology used for better representation of your vector. It can be 1D/2D/3D. It uses a parser \n",
    "# window which moves across the object and pools the data by averaging it (GlobalAveragePooling) or picking max value \n",
    "# (GlobalMaxPooling). Padding is essentially required to take the corner cases into the account.\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 39, 173, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 85, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 41, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 19, 128)        32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 44,602\n",
      "Trainable params: 44,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "1747/1747 [==============================] - 3s 2ms/step\n",
      "Pre-training accuracy: 9.5020%\n"
     ]
    }
   ],
   "source": [
    "# Calculate pre-training accuracy \n",
    "# verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "score = model.evaluate(x_test, y_test, verbose=1) \n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6985 samples, validate on 1747 samples\n",
      "Epoch 1/72\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 2.0351 - accuracy: 0.2732 - val_loss: 1.9897 - val_accuracy: 0.2902\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.98975, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 1.8128 - accuracy: 0.3505 - val_loss: 1.8231 - val_accuracy: 0.3669\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.98975 to 1.82309, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 1.6043 - accuracy: 0.4282 - val_loss: 1.6272 - val_accuracy: 0.4734\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.82309 to 1.62719, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "6985/6985 [==============================] - 38s 5ms/step - loss: 1.4742 - accuracy: 0.4762 - val_loss: 1.5141 - val_accuracy: 0.4888\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.62719 to 1.51406, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "6985/6985 [==============================] - 39s 6ms/step - loss: 1.3890 - accuracy: 0.5049 - val_loss: 1.4729 - val_accuracy: 0.5060\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.51406 to 1.47293, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 1.3083 - accuracy: 0.5402 - val_loss: 1.3821 - val_accuracy: 0.5501\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.47293 to 1.38213, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 7/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 1.2399 - accuracy: 0.5692 - val_loss: 1.3139 - val_accuracy: 0.5638\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.38213 to 1.31390, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 8/72\n",
      "6985/6985 [==============================] - 38s 5ms/step - loss: 1.1867 - accuracy: 0.5800 - val_loss: 1.2607 - val_accuracy: 0.5793\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.31390 to 1.26073, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 9/72\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 1.1337 - accuracy: 0.6017 - val_loss: 1.2277 - val_accuracy: 0.5844\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.26073 to 1.22768, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 10/72\n",
      "6985/6985 [==============================] - 38s 5ms/step - loss: 1.0869 - accuracy: 0.6170 - val_loss: 1.2109 - val_accuracy: 0.5970\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.22768 to 1.21087, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 11/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 1.0569 - accuracy: 0.6276 - val_loss: 1.1934 - val_accuracy: 0.5942\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.21087 to 1.19344, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 12/72\n",
      "6985/6985 [==============================] - 40s 6ms/step - loss: 0.9997 - accuracy: 0.6581 - val_loss: 1.0789 - val_accuracy: 0.6577\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.19344 to 1.07886, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 13/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.9733 - accuracy: 0.6639 - val_loss: 1.1294 - val_accuracy: 0.6285\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.07886\n",
      "Epoch 14/72\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 0.9319 - accuracy: 0.6763 - val_loss: 1.0967 - val_accuracy: 0.6359\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.07886\n",
      "Epoch 15/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.8989 - accuracy: 0.6951 - val_loss: 1.0562 - val_accuracy: 0.6669\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.07886 to 1.05615, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 16/72\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 0.8762 - accuracy: 0.6956 - val_loss: 1.0021 - val_accuracy: 0.6674\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.05615 to 1.00205, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 17/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.8522 - accuracy: 0.7081 - val_loss: 0.9314 - val_accuracy: 0.6863\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.00205 to 0.93141, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 18/72\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 0.8145 - accuracy: 0.7263 - val_loss: 0.9106 - val_accuracy: 0.7115\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.93141 to 0.91063, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 19/72\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 0.7858 - accuracy: 0.7303 - val_loss: 0.8826 - val_accuracy: 0.7355\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.91063 to 0.88261, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 20/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.7573 - accuracy: 0.7394 - val_loss: 0.8717 - val_accuracy: 0.7247\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.88261 to 0.87167, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 21/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.7412 - accuracy: 0.7443 - val_loss: 0.8368 - val_accuracy: 0.7476\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.87167 to 0.83679, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 22/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.7180 - accuracy: 0.7611 - val_loss: 0.7953 - val_accuracy: 0.7665\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.83679 to 0.79534, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 23/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.6964 - accuracy: 0.7652 - val_loss: 0.8088 - val_accuracy: 0.7476\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.79534\n",
      "Epoch 24/72\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 0.7051 - accuracy: 0.7641 - val_loss: 0.7745 - val_accuracy: 0.7636\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.79534 to 0.77450, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 25/72\n",
      "6985/6985 [==============================] - 39s 6ms/step - loss: 0.6836 - accuracy: 0.7654 - val_loss: 0.7566 - val_accuracy: 0.7710\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.77450 to 0.75659, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 26/72\n",
      "6985/6985 [==============================] - 38s 5ms/step - loss: 0.6525 - accuracy: 0.7775 - val_loss: 0.7329 - val_accuracy: 0.7779\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.75659 to 0.73292, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 27/72\n",
      "6985/6985 [==============================] - 41s 6ms/step - loss: 0.6277 - accuracy: 0.7870 - val_loss: 0.6770 - val_accuracy: 0.7934\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.73292 to 0.67698, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 28/72\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 0.6130 - accuracy: 0.7947 - val_loss: 0.6958 - val_accuracy: 0.7836\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.67698\n",
      "Epoch 29/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.5983 - accuracy: 0.7967 - val_loss: 0.6818 - val_accuracy: 0.7939\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.67698\n",
      "Epoch 30/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.6074 - accuracy: 0.7936 - val_loss: 0.6653 - val_accuracy: 0.7916\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.67698 to 0.66533, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 31/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.5926 - accuracy: 0.7980 - val_loss: 0.6447 - val_accuracy: 0.8065\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.66533 to 0.64469, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 32/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.5491 - accuracy: 0.8137 - val_loss: 0.6461 - val_accuracy: 0.7974\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.64469\n",
      "Epoch 33/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.5454 - accuracy: 0.8169 - val_loss: 0.6184 - val_accuracy: 0.8151\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.64469 to 0.61843, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 34/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.5415 - accuracy: 0.8170 - val_loss: 0.6225 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.61843\n",
      "Epoch 35/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.5389 - accuracy: 0.8153 - val_loss: 0.6125 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.61843 to 0.61247, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 36/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.5193 - accuracy: 0.8176 - val_loss: 0.5639 - val_accuracy: 0.8317\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.61247 to 0.56385, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 37/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.5111 - accuracy: 0.8252 - val_loss: 0.5760 - val_accuracy: 0.8277\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.56385\n",
      "Epoch 38/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.5049 - accuracy: 0.8262 - val_loss: 0.5465 - val_accuracy: 0.8386\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.56385 to 0.54652, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 39/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.4935 - accuracy: 0.8324 - val_loss: 0.5525 - val_accuracy: 0.8369\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.54652\n",
      "Epoch 40/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.4876 - accuracy: 0.8348 - val_loss: 0.5480 - val_accuracy: 0.8346\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.54652\n",
      "Epoch 41/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.4738 - accuracy: 0.8392 - val_loss: 0.5138 - val_accuracy: 0.8575\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.54652 to 0.51379, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 42/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.4613 - accuracy: 0.8424 - val_loss: 0.5383 - val_accuracy: 0.8443\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.51379\n",
      "Epoch 43/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.4651 - accuracy: 0.8447 - val_loss: 0.4919 - val_accuracy: 0.8540\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.51379 to 0.49191, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 44/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.4593 - accuracy: 0.8422 - val_loss: 0.5008 - val_accuracy: 0.8523\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.49191\n",
      "Epoch 45/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.4370 - accuracy: 0.8533 - val_loss: 0.4862 - val_accuracy: 0.8575\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.49191 to 0.48625, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 46/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.4298 - accuracy: 0.8517 - val_loss: 0.4805 - val_accuracy: 0.8540\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.48625 to 0.48050, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 47/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.4221 - accuracy: 0.8540 - val_loss: 0.4526 - val_accuracy: 0.8666\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.48050 to 0.45262, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 48/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.4079 - accuracy: 0.8606 - val_loss: 0.4566 - val_accuracy: 0.8569\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.45262\n",
      "Epoch 49/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.4081 - accuracy: 0.8616 - val_loss: 0.4385 - val_accuracy: 0.8769\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.45262 to 0.43852, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 50/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.3836 - accuracy: 0.8702 - val_loss: 0.4568 - val_accuracy: 0.8586\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.43852\n",
      "Epoch 51/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3834 - accuracy: 0.8706 - val_loss: 0.4252 - val_accuracy: 0.8792\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.43852 to 0.42523, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 52/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3896 - accuracy: 0.8650 - val_loss: 0.4491 - val_accuracy: 0.8598\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.42523\n",
      "Epoch 53/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3910 - accuracy: 0.8630 - val_loss: 0.4259 - val_accuracy: 0.8701\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.42523\n",
      "Epoch 54/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3658 - accuracy: 0.8739 - val_loss: 0.4124 - val_accuracy: 0.8781\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.42523 to 0.41235, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 55/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3646 - accuracy: 0.8719 - val_loss: 0.4240 - val_accuracy: 0.8718\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.41235\n",
      "Epoch 56/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3569 - accuracy: 0.8759 - val_loss: 0.4011 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.41235 to 0.40109, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 57/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3596 - accuracy: 0.8762 - val_loss: 0.4328 - val_accuracy: 0.8741\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.40109\n",
      "Epoch 58/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3615 - accuracy: 0.8750 - val_loss: 0.4332 - val_accuracy: 0.8655\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.40109\n",
      "Epoch 59/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3471 - accuracy: 0.8790 - val_loss: 0.3928 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.40109 to 0.39275, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 60/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3315 - accuracy: 0.8862 - val_loss: 0.4150 - val_accuracy: 0.8741\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.39275\n",
      "Epoch 61/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3226 - accuracy: 0.8908 - val_loss: 0.3918 - val_accuracy: 0.8798\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.39275 to 0.39176, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 62/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3230 - accuracy: 0.8879 - val_loss: 0.3737 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.39176 to 0.37365, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 63/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3231 - accuracy: 0.8890 - val_loss: 0.3772 - val_accuracy: 0.8895\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.37365\n",
      "Epoch 64/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3097 - accuracy: 0.8932 - val_loss: 0.3549 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.37365 to 0.35491, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 65/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3055 - accuracy: 0.8939 - val_loss: 0.3721 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.35491\n",
      "Epoch 66/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3150 - accuracy: 0.8939 - val_loss: 0.3667 - val_accuracy: 0.8918\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.35491\n",
      "Epoch 67/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.2981 - accuracy: 0.8966 - val_loss: 0.3691 - val_accuracy: 0.8918\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.35491\n",
      "Epoch 68/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.3029 - accuracy: 0.8948 - val_loss: 0.3578 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.35491\n",
      "Epoch 69/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.2948 - accuracy: 0.8974 - val_loss: 0.3926 - val_accuracy: 0.8769\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.35491\n",
      "Epoch 70/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.2897 - accuracy: 0.9009 - val_loss: 0.3933 - val_accuracy: 0.8798\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.35491\n",
      "Epoch 71/72\n",
      "6985/6985 [==============================] - 36s 5ms/step - loss: 0.2838 - accuracy: 0.9004 - val_loss: 0.3692 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.35491\n",
      "Epoch 72/72\n",
      "6985/6985 [==============================] - 35s 5ms/step - loss: 0.2854 - accuracy: 0.9018 - val_loss: 0.3482 - val_accuracy: 0.8964\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.35491 to 0.34817, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Training completed in time:  0:43:21.272740\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 72\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9411596059799194\n",
      "Testing Accuracy:  0.8963938355445862\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
